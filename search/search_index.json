{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Computer Vision is an exciting new field with the recent advances in deep learning. This documentation serves to compile knowledge acquired through various courses and online readings. Note that I do not claim copyright over any of these code, and you may attribute any of the resources I tagged to them instead.","title":"Introduction"},{"location":"#introduction","text":"Computer Vision is an exciting new field with the recent advances in deep learning. This documentation serves to compile knowledge acquired through various courses and online readings. Note that I do not claim copyright over any of these code, and you may attribute any of the resources I tagged to them instead.","title":"Introduction"},{"location":"augmentation/","text":"Image Augmentation It is hard to obtain photogenic samples of every aspect. Image augmentation enables the auto-generation of new samples from existing ones through random adjustment from rotation, shifts, zoom, brightness etc. The below samples pertains to increasing samples when all samples in classes are balanced from keras_preprocessing.image import ImageDataGenerator train_aug = ImageDataGenerator ( rotation_range = 360 , # Degree range for random rotations width_shift_range = 0.2 , # Range for random horizontal shifts height_shift_range = 0.2 , # Range for random vertical shifts zoom_range = 0.2 , # Range for random zoom horizontal_flip = True , # Randomly flip inputs horizontally vertical_flip = True , # Randomly flip inputs vertically brightness_range = [ 0.5 , 1.5 ]) # we should not augment validation and testing samples val_aug = ImageDataGenerator () test_aug = ImageDataGenerator () After setting the augmentation settings, we will need to decide how to \u201cflow\u201d the data, original samples into the model. In this function, we can also resize the images automatically if necessary. Finally to fit the model, we use the model.fit_generator function so that for every epoch, the full original samples will be augmented randomly on the fly. They will not be stored in memory for obvious reasons. Essentially, there are 3 ways to do this. Flow from Memory First, we can flow the images from memory, i.e., flow , which means we have to load the data in memory first. batch_size = 32 img_size = 100 train_flow = train_aug . flow ( X_train , Y_train , target_size = ( img_size , img_size ), batch_size = batch_size ) val_flow = val_aug . flow ( X_val , Y_val , target_size = ( img_size , img_size ), batch_size = batch_size ) model . fit_generator ( train_flow , steps_per_epoch = 32 , epochs = 15 , verbose = 1 , validation_data = val_flow , use_multiprocessing = True , workers = 2 ) Flow from Dataframe Second, we can flow the images from a directory flow_from_dataframe , where all classes of images are in that single directory. This requires a dataframe which indicates which image correspond to which class. dir = r '/kaggle/input/plant-pathology-2020-fgvc7/images' train_flow = train_aug . flow_from_dataframe ( train_df , directory = dir , x_col = 'image_name' , y_col = [ 'class1' , 'class2' , 'class3' , 'class4' ], class_mode = 'categorical' batch_size = batch_size ) Flow from Directory Third, we can flow the images from a main directory flow_from_directory , where all each class of images are in individual subdirectories. # to include all subdirectories' images, no need specific classes train_flow = train_aug . flow_from_directory ( directory = dir , class_mode = 'categorical' , target_size = ( img_size , img_size ), batch_size = 32 ) # to include specific subdirectories' images, put list of subdirectory names under classes train_flow = train_aug . flow_from_directory ( directory = dir , classes = [ 'subdir1' , 'subdir2' , 'subdir3' ], class_mode = 'categorical' , target_size = ( img_size , img_size ), batch_size = 32 ) Imbalance Data We can also use Kera\u2019s ImageDataGenerator to generate new augmented images when there is class imbalance. Imbalanced data can caused the model to predict the class with highest samples. from keras.preprocessing.image import ImageDataGenerator from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array img = r '/Users/Desktop/post/IMG_20200308_092140.jpg' # load the input image, convert it to a NumPy array, and then # reshape it to have an extra dimension image = load_img ( img ) image = img_to_array ( image ) image = np . expand_dims ( image , axis = 0 ) # augmentation settings aug = ImageDataGenerator ( rotation_range = 15 , width_shift_range = 0.1 , height_shift_range = 0.1 , shear_range = 0.01 , zoom_range = [ 0.9 , 1.25 ], horizontal_flip = True , vertical_flip = False , fill_mode = 'reflect' , data_format = 'channels_last' , brightness_range = [ 0.5 , 1.5 ]) # define input & output imageGen = aug . flow ( image , batch_size = 1 , save_to_dir = r '/Users/Desktop/post/' , save_prefix = \"image\" , save_format = \"jpg\" ) # define number of new augmented samples for count , i in enumerate ( imageGen ): store . append ( i ) if count == 5 : break Resources https://medium.com/datadriveninvestor/keras-imagedatagenerator-methods-an-easy-guide-550ecd3c0a92.","title":"Augmentation"},{"location":"augmentation/#image-augmentation","text":"It is hard to obtain photogenic samples of every aspect. Image augmentation enables the auto-generation of new samples from existing ones through random adjustment from rotation, shifts, zoom, brightness etc. The below samples pertains to increasing samples when all samples in classes are balanced from keras_preprocessing.image import ImageDataGenerator train_aug = ImageDataGenerator ( rotation_range = 360 , # Degree range for random rotations width_shift_range = 0.2 , # Range for random horizontal shifts height_shift_range = 0.2 , # Range for random vertical shifts zoom_range = 0.2 , # Range for random zoom horizontal_flip = True , # Randomly flip inputs horizontally vertical_flip = True , # Randomly flip inputs vertically brightness_range = [ 0.5 , 1.5 ]) # we should not augment validation and testing samples val_aug = ImageDataGenerator () test_aug = ImageDataGenerator () After setting the augmentation settings, we will need to decide how to \u201cflow\u201d the data, original samples into the model. In this function, we can also resize the images automatically if necessary. Finally to fit the model, we use the model.fit_generator function so that for every epoch, the full original samples will be augmented randomly on the fly. They will not be stored in memory for obvious reasons. Essentially, there are 3 ways to do this.","title":"Image Augmentation"},{"location":"augmentation/#flow-from-memory","text":"First, we can flow the images from memory, i.e., flow , which means we have to load the data in memory first. batch_size = 32 img_size = 100 train_flow = train_aug . flow ( X_train , Y_train , target_size = ( img_size , img_size ), batch_size = batch_size ) val_flow = val_aug . flow ( X_val , Y_val , target_size = ( img_size , img_size ), batch_size = batch_size ) model . fit_generator ( train_flow , steps_per_epoch = 32 , epochs = 15 , verbose = 1 , validation_data = val_flow , use_multiprocessing = True , workers = 2 )","title":"Flow from Memory"},{"location":"augmentation/#flow-from-dataframe","text":"Second, we can flow the images from a directory flow_from_dataframe , where all classes of images are in that single directory. This requires a dataframe which indicates which image correspond to which class. dir = r '/kaggle/input/plant-pathology-2020-fgvc7/images' train_flow = train_aug . flow_from_dataframe ( train_df , directory = dir , x_col = 'image_name' , y_col = [ 'class1' , 'class2' , 'class3' , 'class4' ], class_mode = 'categorical' batch_size = batch_size )","title":"Flow from Dataframe"},{"location":"augmentation/#flow-from-directory","text":"Third, we can flow the images from a main directory flow_from_directory , where all each class of images are in individual subdirectories. # to include all subdirectories' images, no need specific classes train_flow = train_aug . flow_from_directory ( directory = dir , class_mode = 'categorical' , target_size = ( img_size , img_size ), batch_size = 32 ) # to include specific subdirectories' images, put list of subdirectory names under classes train_flow = train_aug . flow_from_directory ( directory = dir , classes = [ 'subdir1' , 'subdir2' , 'subdir3' ], class_mode = 'categorical' , target_size = ( img_size , img_size ), batch_size = 32 )","title":"Flow from Directory"},{"location":"augmentation/#imbalance-data","text":"We can also use Kera\u2019s ImageDataGenerator to generate new augmented images when there is class imbalance. Imbalanced data can caused the model to predict the class with highest samples. from keras.preprocessing.image import ImageDataGenerator from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array img = r '/Users/Desktop/post/IMG_20200308_092140.jpg' # load the input image, convert it to a NumPy array, and then # reshape it to have an extra dimension image = load_img ( img ) image = img_to_array ( image ) image = np . expand_dims ( image , axis = 0 ) # augmentation settings aug = ImageDataGenerator ( rotation_range = 15 , width_shift_range = 0.1 , height_shift_range = 0.1 , shear_range = 0.01 , zoom_range = [ 0.9 , 1.25 ], horizontal_flip = True , vertical_flip = False , fill_mode = 'reflect' , data_format = 'channels_last' , brightness_range = [ 0.5 , 1.5 ]) # define input & output imageGen = aug . flow ( image , batch_size = 1 , save_to_dir = r '/Users/Desktop/post/' , save_prefix = \"image\" , save_format = \"jpg\" ) # define number of new augmented samples for count , i in enumerate ( imageGen ): store . append ( i ) if count == 5 : break","title":"Imbalance Data"},{"location":"augmentation/#resources","text":"https://medium.com/datadriveninvestor/keras-imagedatagenerator-methods-an-easy-guide-550ecd3c0a92.","title":"Resources"},{"location":"basics/","text":"Basics of Images Convert to Array Using Numpy (1) import numpy as np import matplotlib.pyplot as plt imgArr = np . asarray ( 'imagepath' ) plt . imshow ( pic_arr ) Using Numpy (2) Sometimes, a Type error will be prompted using cv2.rectangle() when the input array is the usual numpy array. We should use the following instead. import numpy as np imgArr = np . ascontiguousarray ( 'imagepath' ) Using OpenCV import cv2 imgArr = cv2 . imread ( 'imagepath' ) cv2 . imshow ( 'image' , img ) # Wait for something on keyboard to be pressed to close window. # 0 refers to 0 miliseconds of waiting cv2 . waitKey ( 0 ) From base64 string import base64 import cv2 npArr = np . fromstring ( base64 . b64decode ( encodedImage ), np . uint8 ) imgArr = cv2 . imdecode ( npArr , cv2 . IMREAD_ANYCOLOR ) Saving Images cv2 . imwrite ( 'my_new_picture.jpg' , imgArr ) Resizing Imagesw Resizing by a specific scale img = cv2 . imread ( 'imagepath' , cv2 . IMREAD_UNCHANGED ) scale = 0.6 # percent of original size width = int ( img . shape [ 1 ] * scale ) height = int ( img . shape [ 0 ] * scale ) resized = cv2 . resize ( img , ( width , height ), interpolation = cv2 . INTER_AREA ) Resizing by specific height def img_scaling ( frame , new_height = 600 ): ''' rescale image based on a fixed height, and width with same aspect ratio Parameters ---------- frame (array): image array Returns ------- new_width (int): size of new width new_height (int): size of new height ''' width = frame . shape [ 1 ] height = frame . shape [ 0 ] if height > new_height : scale = new_height / height new_width = int ( width * scale ) else : new_width = width new_height = height return new_width , new_height new_width , new_height = img_scaling ( frame ) resized = cv2 . resize ( img , ( new_width , new_height ), interpolation = cv2 . INTER_AREA ) Drawing on Images One of the most important reason to draw on images is to draw bounding boxes representing the prediction output. rectangles # pt1 = top left # pt2 = bottom right cv2 . rectangle ( imgArr , pt1 = ( 384 , 0 ), pt2 = ( 510 , 128 ), \\ color = ( 0 , 255 , 0 ), thickness = 5 ) Here's a typical example function from xiaochus's YOLO on how it is used. def draw ( image , boxes , scores , classes , all_classes ): '''Draw the boxes on the image. Argument: image: original image. boxes: ndarray, boxes of objects. classes: ndarray, classes of objects. scores: ndarray, scores of objects. all_classes: all classes name. ''' for box , score , cl in zip ( boxes , scores , classes ): x , y , w , h = box top = max ( 0 , np . floor ( x + 0.5 ) . astype ( int )) left = max ( 0 , np . floor ( y + 0.5 ) . astype ( int )) right = min ( image . shape [ 1 ], np . floor ( x + w + 0.5 ) . astype ( int )) bottom = min ( image . shape [ 0 ], np . floor ( y + h + 0.5 ) . astype ( int )) cv2 . rectangle ( image , ( top , left ), ( right , bottom ), ( 255 , 0 , 0 ), 2 ) cv2 . putText ( image , '{0} {1:.2f}' . format ( all_classes [ cl ], score ), ( top , left - 6 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.6 , ( 0 , 0 , 255 ), 1 , cv2 . LINE_AA ) print ( 'class: {0}, score: {1:.2f}' . format ( all_classes [ cl ], score )) print ( 'box coordinate x,y,w,h: {0}' . format ( box )) Wait & Break This is not exactly pythonic, so it means it is not as easy to decipher. 0xFF is an 8 bit binary mask that forces the result from waitKey() to be an integer of maximum 255, which is what a character in the keyboard can go till. ord(char) returns the character in integers which will also be of maximum 255. Hence by comparing the integer to the ord(char) value, we can check for a key pressed event and break the loop. # stop when character \"q\" is pressed if cv2 . waitKey ( 0 ) & 0xFF == ord ( 'q' ): break # stop when \"ESC\" key is pressed if cv2 . waitKey ( 20 ) & 0xFF == 27 : break # Once script is done, its usually good practice to call this line # It closes all windows (just in case you have multiple windows called) cv2 . destroyAllWindows ()","title":"Image Basics"},{"location":"basics/#basics-of-images","text":"","title":"Basics of Images"},{"location":"basics/#convert-to-array","text":"Using Numpy (1) import numpy as np import matplotlib.pyplot as plt imgArr = np . asarray ( 'imagepath' ) plt . imshow ( pic_arr ) Using Numpy (2) Sometimes, a Type error will be prompted using cv2.rectangle() when the input array is the usual numpy array. We should use the following instead. import numpy as np imgArr = np . ascontiguousarray ( 'imagepath' ) Using OpenCV import cv2 imgArr = cv2 . imread ( 'imagepath' ) cv2 . imshow ( 'image' , img ) # Wait for something on keyboard to be pressed to close window. # 0 refers to 0 miliseconds of waiting cv2 . waitKey ( 0 ) From base64 string import base64 import cv2 npArr = np . fromstring ( base64 . b64decode ( encodedImage ), np . uint8 ) imgArr = cv2 . imdecode ( npArr , cv2 . IMREAD_ANYCOLOR )","title":"Convert to Array"},{"location":"basics/#saving-images","text":"cv2 . imwrite ( 'my_new_picture.jpg' , imgArr )","title":"Saving Images"},{"location":"basics/#resizing-imagesw","text":"Resizing by a specific scale img = cv2 . imread ( 'imagepath' , cv2 . IMREAD_UNCHANGED ) scale = 0.6 # percent of original size width = int ( img . shape [ 1 ] * scale ) height = int ( img . shape [ 0 ] * scale ) resized = cv2 . resize ( img , ( width , height ), interpolation = cv2 . INTER_AREA ) Resizing by specific height def img_scaling ( frame , new_height = 600 ): ''' rescale image based on a fixed height, and width with same aspect ratio Parameters ---------- frame (array): image array Returns ------- new_width (int): size of new width new_height (int): size of new height ''' width = frame . shape [ 1 ] height = frame . shape [ 0 ] if height > new_height : scale = new_height / height new_width = int ( width * scale ) else : new_width = width new_height = height return new_width , new_height new_width , new_height = img_scaling ( frame ) resized = cv2 . resize ( img , ( new_width , new_height ), interpolation = cv2 . INTER_AREA )","title":"Resizing Imagesw"},{"location":"basics/#drawing-on-images","text":"One of the most important reason to draw on images is to draw bounding boxes representing the prediction output. rectangles # pt1 = top left # pt2 = bottom right cv2 . rectangle ( imgArr , pt1 = ( 384 , 0 ), pt2 = ( 510 , 128 ), \\ color = ( 0 , 255 , 0 ), thickness = 5 ) Here's a typical example function from xiaochus's YOLO on how it is used. def draw ( image , boxes , scores , classes , all_classes ): '''Draw the boxes on the image. Argument: image: original image. boxes: ndarray, boxes of objects. classes: ndarray, classes of objects. scores: ndarray, scores of objects. all_classes: all classes name. ''' for box , score , cl in zip ( boxes , scores , classes ): x , y , w , h = box top = max ( 0 , np . floor ( x + 0.5 ) . astype ( int )) left = max ( 0 , np . floor ( y + 0.5 ) . astype ( int )) right = min ( image . shape [ 1 ], np . floor ( x + w + 0.5 ) . astype ( int )) bottom = min ( image . shape [ 0 ], np . floor ( y + h + 0.5 ) . astype ( int )) cv2 . rectangle ( image , ( top , left ), ( right , bottom ), ( 255 , 0 , 0 ), 2 ) cv2 . putText ( image , '{0} {1:.2f}' . format ( all_classes [ cl ], score ), ( top , left - 6 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.6 , ( 0 , 0 , 255 ), 1 , cv2 . LINE_AA ) print ( 'class: {0}, score: {1:.2f}' . format ( all_classes [ cl ], score )) print ( 'box coordinate x,y,w,h: {0}' . format ( box ))","title":"Drawing on Images"},{"location":"basics/#wait-break","text":"This is not exactly pythonic, so it means it is not as easy to decipher. 0xFF is an 8 bit binary mask that forces the result from waitKey() to be an integer of maximum 255, which is what a character in the keyboard can go till. ord(char) returns the character in integers which will also be of maximum 255. Hence by comparing the integer to the ord(char) value, we can check for a key pressed event and break the loop. # stop when character \"q\" is pressed if cv2 . waitKey ( 0 ) & 0xFF == ord ( 'q' ): break # stop when \"ESC\" key is pressed if cv2 . waitKey ( 20 ) & 0xFF == 27 : break # Once script is done, its usually good practice to call this line # It closes all windows (just in case you have multiple windows called) cv2 . destroyAllWindows ()","title":"Wait &amp; Break"},{"location":"cnn/","text":"Convolutional Neural Networks","title":"CNN"},{"location":"cnn/#convolutional-neural-networks","text":"","title":"Convolutional Neural Networks"},{"location":"datasets/","text":"Datasets Data is always needed to make a good model. This pages provides sites to download open datasets to train your model. CVonline: Image Databases Kaggle","title":"Datasets"},{"location":"datasets/#datasets","text":"Data is always needed to make a good model. This pages provides sites to download open datasets to train your model. CVonline: Image Databases Kaggle","title":"Datasets"},{"location":"install/","text":"Installation Installations for computer vision can prove to tricky, from nvidia GPU to certain libraries compiled from other languages, it can turn out to extremely time-consuming. OpenCV Using pip install opencv-python installs the official version of cv2. However, there are some contribution libraries which are missing because of certain issues; to install a complete package, use pip install opencv-contrib-python instead. EfficientNet pip install efficientnet","title":"Installation"},{"location":"install/#installation","text":"Installations for computer vision can prove to tricky, from nvidia GPU to certain libraries compiled from other languages, it can turn out to extremely time-consuming.","title":"Installation"},{"location":"install/#opencv","text":"Using pip install opencv-python installs the official version of cv2. However, there are some contribution libraries which are missing because of certain issues; to install a complete package, use pip install opencv-contrib-python instead.","title":"OpenCV"},{"location":"install/#efficientnet","text":"pip install efficientnet","title":"EfficientNet"},{"location":"tracking/","text":"Object Tracking There are various ready made tracking APIs in opencv that we can call directly from. Note that they are only available in the opencv-contrib-python version. This requires a user to mark out an object of interest first. There are various advantages and disadvantages of each algorithm, with KCF as a good start point, while Boosting and MIL as weaker ones. import cv2 def ask_for_tracker (): print ( \"Welcome! What Tracker API would you like to use?\" ) print ( \"Enter 0 for BOOSTING: \" ) print ( \"Enter 1 for MIL: \" ) print ( \"Enter 2 for KCF: \" ) print ( \"Enter 3 for TLD: \" ) print ( \"Enter 4 for MEDIANFLOW: \" ) choice = input ( \"Please select your tracker: \" ) if choice == '0' : tracker = cv2 . TrackerBoosting_create () if choice == '1' : tracker = cv2 . TrackerMIL_create () if choice == '2' : tracker = cv2 . TrackerKCF_create () if choice == '3' : tracker = cv2 . TrackerTLD_create () if choice == '4' : tracker = cv2 . TrackerMedianFlow_create () return tracker tracker = ask_for_tracker () tracker_name = str ( tracker ) . split ()[ 0 ][ 1 :] cap = cv2 . VideoCapture ( 0 ) ret , frame = cap . read () # Special function allows us to draw on the very first frame our desired ROI # hit enter roi = cv2 . selectROI ( frame , False ) ret = tracker . init ( frame , roi ) while True : ret , frame = cap . read () # Update tracker success , roi = tracker . update ( frame ) # roi variable is a tuple of 4 floats # We need each value and we need them as integers ( x , y , w , h ) = tuple ( map ( int , roi )) # Draw Rectangle as Tracker moves if success : p1 = ( x , y ) p2 = ( x + w , y + h ) cv2 . rectangle ( frame , p1 , p2 , ( 0 , 255 , 0 ), 3 ) else : # Tracking failure cv2 . putText ( frame , \"Failure to Detect Tracking!!\" , ( 100 , 200 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 ,( 0 , 0 , 255 ), 3 ) # Display tracker type on frame cv2 . putText ( frame , tracker_name , ( 20 , 400 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 , ( 0 , 255 , 0 ), 3 ); # Display result cv2 . imshow ( tracker_name , frame ) # Exit if ESC pressed k = cv2 . waitKey ( 1 ) & 0xff if k == 27 : break cap . release () cv2 . destroyAllWindows ()","title":"Object Tracking"},{"location":"tracking/#object-tracking","text":"There are various ready made tracking APIs in opencv that we can call directly from. Note that they are only available in the opencv-contrib-python version. This requires a user to mark out an object of interest first. There are various advantages and disadvantages of each algorithm, with KCF as a good start point, while Boosting and MIL as weaker ones. import cv2 def ask_for_tracker (): print ( \"Welcome! What Tracker API would you like to use?\" ) print ( \"Enter 0 for BOOSTING: \" ) print ( \"Enter 1 for MIL: \" ) print ( \"Enter 2 for KCF: \" ) print ( \"Enter 3 for TLD: \" ) print ( \"Enter 4 for MEDIANFLOW: \" ) choice = input ( \"Please select your tracker: \" ) if choice == '0' : tracker = cv2 . TrackerBoosting_create () if choice == '1' : tracker = cv2 . TrackerMIL_create () if choice == '2' : tracker = cv2 . TrackerKCF_create () if choice == '3' : tracker = cv2 . TrackerTLD_create () if choice == '4' : tracker = cv2 . TrackerMedianFlow_create () return tracker tracker = ask_for_tracker () tracker_name = str ( tracker ) . split ()[ 0 ][ 1 :] cap = cv2 . VideoCapture ( 0 ) ret , frame = cap . read () # Special function allows us to draw on the very first frame our desired ROI # hit enter roi = cv2 . selectROI ( frame , False ) ret = tracker . init ( frame , roi ) while True : ret , frame = cap . read () # Update tracker success , roi = tracker . update ( frame ) # roi variable is a tuple of 4 floats # We need each value and we need them as integers ( x , y , w , h ) = tuple ( map ( int , roi )) # Draw Rectangle as Tracker moves if success : p1 = ( x , y ) p2 = ( x + w , y + h ) cv2 . rectangle ( frame , p1 , p2 , ( 0 , 255 , 0 ), 3 ) else : # Tracking failure cv2 . putText ( frame , \"Failure to Detect Tracking!!\" , ( 100 , 200 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 ,( 0 , 0 , 255 ), 3 ) # Display tracker type on frame cv2 . putText ( frame , tracker_name , ( 20 , 400 ), cv2 . FONT_HERSHEY_SIMPLEX , 1 , ( 0 , 255 , 0 ), 3 ); # Display result cv2 . imshow ( tracker_name , frame ) # Exit if ESC pressed k = cv2 . waitKey ( 1 ) & 0xff if k == 27 : break cap . release () cv2 . destroyAllWindows ()","title":"Object Tracking"},{"location":"transfer_learning/","text":"Transfer Learning For CNN, because of the huge research done, and the complexity in architecture, we can use existing ones, with pretrained weights. For transfer learning for image recognition, the defacto is imagenet, whereby we can specify it under the weights argument. EfficientNet EfficientNet is developed by Google in 2019. It is able to achieve a high accuracy with less parameters through a novel compound scaling method [image (e)] versus traditional methods [images (b-d)]. import efficientnet.tfkeras as efn def model ( input_shape , classes ): ''' transfer learning from imagenet's weights, using Google's efficientnet7 architecture top layer (include_top) is removed as the number of classes is changed ''' base = efn . EfficientNetB7 ( input_shape = input_shape , weights = 'imagenet' , include_top = False ) model = Sequential () model . add ( base ) model . add ( GlobalAveragePooling2D ()) model . add ( Dense ( classes , activation = 'softmax' )) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return model # alternatively... def model ( input_shape , classes ): model = efn . EfficientNetB3 ( input_shape = input_shape , weights = 'imagenet' , include_top = False ) x = model . output x = Flatten ()( x ) x = Dropout ( 0.5 )( x ) output_layer = Dense ( classes , activation = 'softmax' )( x ) model = Model ( inputs = model . input , outputs = output_layer ) model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) return model YOLO YOLO (You Only Look Once) is an object detection framework that works extremely fast compared to other existing frameworks. EfficientDet EfficientDet is also another architecture, with the backbone using EfficientNet, developed by Google in 2019. https://github.com/Star-Clouds/CenterFace AutoML Google Neural Architecture Search Here's a nice instructional guide on how to use","title":"Transfer Learning"},{"location":"transfer_learning/#transfer-learning","text":"For CNN, because of the huge research done, and the complexity in architecture, we can use existing ones, with pretrained weights. For transfer learning for image recognition, the defacto is imagenet, whereby we can specify it under the weights argument.","title":"Transfer Learning"},{"location":"transfer_learning/#efficientnet","text":"EfficientNet is developed by Google in 2019. It is able to achieve a high accuracy with less parameters through a novel compound scaling method [image (e)] versus traditional methods [images (b-d)]. import efficientnet.tfkeras as efn def model ( input_shape , classes ): ''' transfer learning from imagenet's weights, using Google's efficientnet7 architecture top layer (include_top) is removed as the number of classes is changed ''' base = efn . EfficientNetB7 ( input_shape = input_shape , weights = 'imagenet' , include_top = False ) model = Sequential () model . add ( base ) model . add ( GlobalAveragePooling2D ()) model . add ( Dense ( classes , activation = 'softmax' )) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return model # alternatively... def model ( input_shape , classes ): model = efn . EfficientNetB3 ( input_shape = input_shape , weights = 'imagenet' , include_top = False ) x = model . output x = Flatten ()( x ) x = Dropout ( 0.5 )( x ) output_layer = Dense ( classes , activation = 'softmax' )( x ) model = Model ( inputs = model . input , outputs = output_layer ) model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) return model","title":"EfficientNet"},{"location":"transfer_learning/#yolo","text":"YOLO (You Only Look Once) is an object detection framework that works extremely fast compared to other existing frameworks.","title":"YOLO"},{"location":"transfer_learning/#efficientdet","text":"EfficientDet is also another architecture, with the backbone using EfficientNet, developed by Google in 2019. https://github.com/Star-Clouds/CenterFace","title":"EfficientDet"},{"location":"transfer_learning/#automl","text":"Google Neural Architecture Search Here's a nice instructional guide on how to use","title":"AutoML"},{"location":"videos/","text":"Videos From Webcam import cv2 # Connects to your computer's default camera cap = cv2 . VideoCapture ( 0 ) while True : # Capture frame-by-frame ret , frame = cap . read () # Our operations on the frame come here gray = cv2 . cvtColor ( frame , cv2 . COLOR_BGR2GRAY ) cv2 . imshow ( 'frame' , gray ) # quit with \"q\" if cv2 . waitKey ( 1 ) & 0xFF == ord ( 'q' ): break # When everything done, release the capture and destroy the windows cap . release () cv2 . destroyAllWindows () From File import cv2 import time cap = cv2 . VideoCapture ( '../DATA/video_capture.mp4' ) # FRAMES PER SECOND fps = 25 # check for video file if cap . isOpened () == False : print ( \"Error opening the video file\" ) # While the video is opened while cap . isOpened (): # Read the video file ret , frame = cap . read () # If we got frames, show them. if ret == True : # Display the frame at same frame rate of recording time . sleep ( 1 / fps ) cv2 . imshow ( 'frame' , frame ) if cv2 . waitKey ( 25 ) & 0xFF == ord ( 'q' ): break # automatically break this whole loop if the video is over else : break cap . release () # Closes all the frames cv2 . destroyAllWindows () Get Latest Frames OpenCV's videocapture's read() only reads the frames sequentially. However, we might want to acquire the latest frame after processing the previous one, else the latency will quickly build up overtime. A helpful code snippet from stackoverflow can help to resolve this: https://stackoverflow.com/questions/51722319/skip-frames-and-seek-to-end-of-rtsp-stream-in-opencv import cv2 import threading from threading import lock class Camera : \"\"\"Read the latest frame from RTSP video stream. A daemon thread is produced that runs the stream in the background. When required, getFrame() will grab the latest copy of the frame from the thread. \"\"\" last_frame = None last_ready = None lock = Lock () def __init__ ( self , rtsp_link ): capture = cv2 . VideoCapture ( rtsp_link ) thread = threading . Thread ( target = self . rtsp_cam_buffer , args = ( capture ,), name = \"rtsp_read_thread\" ) thread . daemon = True thread . start () def rtsp_cam_buffer ( self , capture ): while True : with self . lock : self . last_ready , self . last_frame = capture . read () def getFrame ( self ): if ( self . last_ready is not None ) and ( self . last_frame is not None ): return self . last_frame . copy () else : return None This is how we call the class in. uri = 'mask.mp4' capture = Camera ( uri ) while True : frame = capture . getFrame () if frame is not None : cv2 . imshow ( 'frame' , frame ) else : print ( 'frame is None' ) if cv2 . waitKey ( 25 ) & 0xFF == ord ( 'q' ): break","title":"Videos"},{"location":"videos/#videos","text":"","title":"Videos"},{"location":"videos/#from-webcam","text":"import cv2 # Connects to your computer's default camera cap = cv2 . VideoCapture ( 0 ) while True : # Capture frame-by-frame ret , frame = cap . read () # Our operations on the frame come here gray = cv2 . cvtColor ( frame , cv2 . COLOR_BGR2GRAY ) cv2 . imshow ( 'frame' , gray ) # quit with \"q\" if cv2 . waitKey ( 1 ) & 0xFF == ord ( 'q' ): break # When everything done, release the capture and destroy the windows cap . release () cv2 . destroyAllWindows ()","title":"From Webcam"},{"location":"videos/#from-file","text":"import cv2 import time cap = cv2 . VideoCapture ( '../DATA/video_capture.mp4' ) # FRAMES PER SECOND fps = 25 # check for video file if cap . isOpened () == False : print ( \"Error opening the video file\" ) # While the video is opened while cap . isOpened (): # Read the video file ret , frame = cap . read () # If we got frames, show them. if ret == True : # Display the frame at same frame rate of recording time . sleep ( 1 / fps ) cv2 . imshow ( 'frame' , frame ) if cv2 . waitKey ( 25 ) & 0xFF == ord ( 'q' ): break # automatically break this whole loop if the video is over else : break cap . release () # Closes all the frames cv2 . destroyAllWindows ()","title":"From File"},{"location":"videos/#get-latest-frames","text":"OpenCV's videocapture's read() only reads the frames sequentially. However, we might want to acquire the latest frame after processing the previous one, else the latency will quickly build up overtime. A helpful code snippet from stackoverflow can help to resolve this: https://stackoverflow.com/questions/51722319/skip-frames-and-seek-to-end-of-rtsp-stream-in-opencv import cv2 import threading from threading import lock class Camera : \"\"\"Read the latest frame from RTSP video stream. A daemon thread is produced that runs the stream in the background. When required, getFrame() will grab the latest copy of the frame from the thread. \"\"\" last_frame = None last_ready = None lock = Lock () def __init__ ( self , rtsp_link ): capture = cv2 . VideoCapture ( rtsp_link ) thread = threading . Thread ( target = self . rtsp_cam_buffer , args = ( capture ,), name = \"rtsp_read_thread\" ) thread . daemon = True thread . start () def rtsp_cam_buffer ( self , capture ): while True : with self . lock : self . last_ready , self . last_frame = capture . read () def getFrame ( self ): if ( self . last_ready is not None ) and ( self . last_frame is not None ): return self . last_frame . copy () else : return None This is how we call the class in. uri = 'mask.mp4' capture = Camera ( uri ) while True : frame = capture . getFrame () if frame is not None : cv2 . imshow ( 'frame' , frame ) else : print ( 'frame is None' ) if cv2 . waitKey ( 25 ) & 0xFF == ord ( 'q' ): break","title":"Get Latest Frames"}]}