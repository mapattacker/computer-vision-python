{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Computer Vision is an exciting new field with the recent advances in deep learning. This documentation serves to compile knowledge acquired through various courses and online readings. Note that I do not claim copyright over any of these code, and you may attribute any of the resources I tagged to them instead.","title":"Introduction"},{"location":"#introduction","text":"Computer Vision is an exciting new field with the recent advances in deep learning. This documentation serves to compile knowledge acquired through various courses and online readings. Note that I do not claim copyright over any of these code, and you may attribute any of the resources I tagged to them instead.","title":"Introduction"},{"location":"augmentation/","text":"Image Augmentation It is hard to obtain photogenic samples of every aspect. Image augmentation enables the auto-generation of new samples from existing ones through random adjustment from rotation, shifts, zoom, brightness etc. The below samples pertains to increasing samples when all samples in classes are balanced from keras_preprocessing.image import ImageDataGenerator train_aug = ImageDataGenerator ( rotation_range = 360 , # Degree range for random rotations width_shift_range = 0.2 , # Range for random horizontal shifts height_shift_range = 0.2 , # Range for random vertical shifts zoom_range = 0.2 , # Range for random zoom horizontal_flip = True , # Randomly flip inputs horizontally vertical_flip = True , # Randomly flip inputs vertically brightness_range = [ 0.5 , 1.5 ]) # we should not augment validation and testing samples val_aug = ImageDataGenerator () test_aug = ImageDataGenerator () After setting the augmentation settings, we will need to decide how to \u201cflow\u201d the data, original samples into the model. In this function, we can also resize the images automatically if necessary. Finally to fit the model, we use the model.fit_generator function so that for every epoch, the full original samples will be augmented randomly on the fly. They will not be stored in memory for obvious reasons. Essentially, there are 3 ways to do this. Flow from Memory First, we can flow the images from memory, i.e., flow , which means we have to load the data in memory first. batch_size = 32 img_size = 100 train_flow = train_aug . flow ( X_train , Y_train , target_size = ( img_size , img_size ), batch_size = batch_size ) val_flow = val_aug . flow ( X_val , Y_val , target_size = ( img_size , img_size ), batch_size = batch_size ) model . fit_generator ( train_flow , steps_per_epoch = 32 , epochs = 15 , verbose = 1 , validation_data = val_flow , use_multiprocessing = True , workers = 2 ) Flow from Dataframe Second, we can flow the images from a directory flow_from_dataframe , where all classes of images are in that single directory. This requires a dataframe which indicates which image correspond to which class. dir = r '/kaggle/input/plant-pathology-2020-fgvc7/images' train_flow = train_aug . flow_from_dataframe ( train_df , directory = dir , x_col = 'image_name' , y_col = [ 'class1' , 'class2' , 'class3' , 'class4' ], class_mode = 'categorical' batch_size = batch_size ) Flow from Directory Third, we can flow the images from a main directory flow_from_directory , where all each class of images are in individual subdirectories. # to include all subdirectories' images, no need specific classes train_flow = train_aug . flow_from_directory ( directory = dir , class_mode = 'categorical' , target_size = ( img_size , img_size ), batch_size = 32 ) # to include specific subdirectories' images, put list of subdirectory names under classes train_flow = train_aug . flow_from_directory ( directory = dir , classes = [ 'subdir1' , 'subdir2' , 'subdir3' ], class_mode = 'categorical' , target_size = ( img_size , img_size ), batch_size = 32 ) Imbalance Data We can also use Kera\u2019s ImageDataGenerator to generate new augmented images when there is class imbalance. Imbalanced data can caused the model to predict the class with highest samples. from keras.preprocessing.image import ImageDataGenerator from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array img = r '/Users/Desktop/post/IMG_20200308_092140.jpg' # load the input image, convert it to a NumPy array, and then # reshape it to have an extra dimension image = load_img ( img ) image = img_to_array ( image ) image = np . expand_dims ( image , axis = 0 ) # augmentation settings aug = ImageDataGenerator ( rotation_range = 15 , width_shift_range = 0.1 , height_shift_range = 0.1 , shear_range = 0.01 , zoom_range = [ 0.9 , 1.25 ], horizontal_flip = True , vertical_flip = False , fill_mode = 'reflect' , data_format = 'channels_last' , brightness_range = [ 0.5 , 1.5 ]) # define input & output imageGen = aug . flow ( image , batch_size = 1 , save_to_dir = r '/Users/Desktop/post/' , save_prefix = \"image\" , save_format = \"jpg\" ) # define number of new augmented samples for count , i in enumerate ( imageGen ): store . append ( i ) if count == 5 : break Resources https://medium.com/datadriveninvestor/keras-imagedatagenerator-methods-an-easy-guide-550ecd3c0a92.","title":"Augmentation"},{"location":"augmentation/#image-augmentation","text":"It is hard to obtain photogenic samples of every aspect. Image augmentation enables the auto-generation of new samples from existing ones through random adjustment from rotation, shifts, zoom, brightness etc. The below samples pertains to increasing samples when all samples in classes are balanced from keras_preprocessing.image import ImageDataGenerator train_aug = ImageDataGenerator ( rotation_range = 360 , # Degree range for random rotations width_shift_range = 0.2 , # Range for random horizontal shifts height_shift_range = 0.2 , # Range for random vertical shifts zoom_range = 0.2 , # Range for random zoom horizontal_flip = True , # Randomly flip inputs horizontally vertical_flip = True , # Randomly flip inputs vertically brightness_range = [ 0.5 , 1.5 ]) # we should not augment validation and testing samples val_aug = ImageDataGenerator () test_aug = ImageDataGenerator () After setting the augmentation settings, we will need to decide how to \u201cflow\u201d the data, original samples into the model. In this function, we can also resize the images automatically if necessary. Finally to fit the model, we use the model.fit_generator function so that for every epoch, the full original samples will be augmented randomly on the fly. They will not be stored in memory for obvious reasons. Essentially, there are 3 ways to do this.","title":"Image Augmentation"},{"location":"augmentation/#flow-from-memory","text":"First, we can flow the images from memory, i.e., flow , which means we have to load the data in memory first. batch_size = 32 img_size = 100 train_flow = train_aug . flow ( X_train , Y_train , target_size = ( img_size , img_size ), batch_size = batch_size ) val_flow = val_aug . flow ( X_val , Y_val , target_size = ( img_size , img_size ), batch_size = batch_size ) model . fit_generator ( train_flow , steps_per_epoch = 32 , epochs = 15 , verbose = 1 , validation_data = val_flow , use_multiprocessing = True , workers = 2 )","title":"Flow from Memory"},{"location":"augmentation/#flow-from-dataframe","text":"Second, we can flow the images from a directory flow_from_dataframe , where all classes of images are in that single directory. This requires a dataframe which indicates which image correspond to which class. dir = r '/kaggle/input/plant-pathology-2020-fgvc7/images' train_flow = train_aug . flow_from_dataframe ( train_df , directory = dir , x_col = 'image_name' , y_col = [ 'class1' , 'class2' , 'class3' , 'class4' ], class_mode = 'categorical' batch_size = batch_size )","title":"Flow from Dataframe"},{"location":"augmentation/#flow-from-directory","text":"Third, we can flow the images from a main directory flow_from_directory , where all each class of images are in individual subdirectories. # to include all subdirectories' images, no need specific classes train_flow = train_aug . flow_from_directory ( directory = dir , class_mode = 'categorical' , target_size = ( img_size , img_size ), batch_size = 32 ) # to include specific subdirectories' images, put list of subdirectory names under classes train_flow = train_aug . flow_from_directory ( directory = dir , classes = [ 'subdir1' , 'subdir2' , 'subdir3' ], class_mode = 'categorical' , target_size = ( img_size , img_size ), batch_size = 32 )","title":"Flow from Directory"},{"location":"augmentation/#imbalance-data","text":"We can also use Kera\u2019s ImageDataGenerator to generate new augmented images when there is class imbalance. Imbalanced data can caused the model to predict the class with highest samples. from keras.preprocessing.image import ImageDataGenerator from keras.preprocessing.image import load_img from keras.preprocessing.image import img_to_array img = r '/Users/Desktop/post/IMG_20200308_092140.jpg' # load the input image, convert it to a NumPy array, and then # reshape it to have an extra dimension image = load_img ( img ) image = img_to_array ( image ) image = np . expand_dims ( image , axis = 0 ) # augmentation settings aug = ImageDataGenerator ( rotation_range = 15 , width_shift_range = 0.1 , height_shift_range = 0.1 , shear_range = 0.01 , zoom_range = [ 0.9 , 1.25 ], horizontal_flip = True , vertical_flip = False , fill_mode = 'reflect' , data_format = 'channels_last' , brightness_range = [ 0.5 , 1.5 ]) # define input & output imageGen = aug . flow ( image , batch_size = 1 , save_to_dir = r '/Users/Desktop/post/' , save_prefix = \"image\" , save_format = \"jpg\" ) # define number of new augmented samples for count , i in enumerate ( imageGen ): store . append ( i ) if count == 5 : break","title":"Imbalance Data"},{"location":"augmentation/#resources","text":"https://medium.com/datadriveninvestor/keras-imagedatagenerator-methods-an-easy-guide-550ecd3c0a92.","title":"Resources"},{"location":"basics/","text":"Basics of Images Convert to Array Using Numpy (1) import numpy as np import matplotlib.pyplot as plt imgArr = np . asarray ( 'imagepath' ) plt . imshow ( pic_arr ) Using Numpy (2) Sometimes, a Type error will be prompted using cv2.rectangle() when the input array is the usual numpy array. We should use the following instead. import numpy as np imgArr = np . ascontiguousarray ( 'imagepath' ) Using OpenCV import cv2 imgArr = cv2 . imread ( 'imagepath' ) cv2 . imshow ( 'image' , img ) # Wait for something on keyboard to be pressed to close window. # 0 refers to 0 miliseconds of waiting cv2 . waitKey ( 0 ) From base64 string import base64 import cv2 npArr = np . fromstring ( base64 . b64decode ( encodedImage ), np . uint8 ) imgArr = cv2 . imdecode ( npArr , cv2 . IMREAD_ANYCOLOR ) Saving Images cv2 . imwrite ( 'my_new_picture.jpg' , imgArr ) Drawing on Images One of the most important reason to draw on images is to draw bounding boxes representing the prediction output. rectangles # pt1 = top left # pt2 = bottom right cv2 . rectangle ( imgArr , pt1 = ( 384 , 0 ), pt2 = ( 510 , 128 ), \\ color = ( 0 , 255 , 0 ), thickness = 5 ) Here's a typical example function from xiaochus's YOLO on how it is used. def draw ( image , boxes , scores , classes , all_classes ): '''Draw the boxes on the image. Argument: image: original image. boxes: ndarray, boxes of objects. classes: ndarray, classes of objects. scores: ndarray, scores of objects. all_classes: all classes name. ''' for box , score , cl in zip ( boxes , scores , classes ): x , y , w , h = box top = max ( 0 , np . floor ( x + 0.5 ) . astype ( int )) left = max ( 0 , np . floor ( y + 0.5 ) . astype ( int )) right = min ( image . shape [ 1 ], np . floor ( x + w + 0.5 ) . astype ( int )) bottom = min ( image . shape [ 0 ], np . floor ( y + h + 0.5 ) . astype ( int )) cv2 . rectangle ( image , ( top , left ), ( right , bottom ), ( 255 , 0 , 0 ), 2 ) cv2 . putText ( image , '{0} {1:.2f}' . format ( all_classes [ cl ], score ), ( top , left - 6 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.6 , ( 0 , 0 , 255 ), 1 , cv2 . LINE_AA ) print ( 'class: {0}, score: {1:.2f}' . format ( all_classes [ cl ], score )) print ( 'box coordinate x,y,w,h: {0}' . format ( box )) Wait & Break This is not exactly pythonic, so it means it is not as easy to decipher. 0xFF is an 8 bit binary mask that forces the result from waitKey() to be an integer of maximum 255, which is what a character in the keyboard can go till. ord(char) returns the character in integers which will also be of maximum 255. Hence by comparing the integer to the ord(char) value, we can check for a key pressed event and break the loop. # stop when character \"q\" is pressed if cv2 . waitKey ( 0 ) & 0xFF == ord ( 'q' ): break # stop when \"ESC\" key is pressed if cv2 . waitKey ( 20 ) & 0xFF == 27 : break # Once script is done, its usually good practice to call this line # It closes all windows (just in case you have multiple windows called) cv2 . destroyAllWindows ()","title":"Image Basics"},{"location":"basics/#basics-of-images","text":"","title":"Basics of Images"},{"location":"basics/#convert-to-array","text":"Using Numpy (1) import numpy as np import matplotlib.pyplot as plt imgArr = np . asarray ( 'imagepath' ) plt . imshow ( pic_arr ) Using Numpy (2) Sometimes, a Type error will be prompted using cv2.rectangle() when the input array is the usual numpy array. We should use the following instead. import numpy as np imgArr = np . ascontiguousarray ( 'imagepath' ) Using OpenCV import cv2 imgArr = cv2 . imread ( 'imagepath' ) cv2 . imshow ( 'image' , img ) # Wait for something on keyboard to be pressed to close window. # 0 refers to 0 miliseconds of waiting cv2 . waitKey ( 0 ) From base64 string import base64 import cv2 npArr = np . fromstring ( base64 . b64decode ( encodedImage ), np . uint8 ) imgArr = cv2 . imdecode ( npArr , cv2 . IMREAD_ANYCOLOR )","title":"Convert to Array"},{"location":"basics/#saving-images","text":"cv2 . imwrite ( 'my_new_picture.jpg' , imgArr )","title":"Saving Images"},{"location":"basics/#drawing-on-images","text":"One of the most important reason to draw on images is to draw bounding boxes representing the prediction output. rectangles # pt1 = top left # pt2 = bottom right cv2 . rectangle ( imgArr , pt1 = ( 384 , 0 ), pt2 = ( 510 , 128 ), \\ color = ( 0 , 255 , 0 ), thickness = 5 ) Here's a typical example function from xiaochus's YOLO on how it is used. def draw ( image , boxes , scores , classes , all_classes ): '''Draw the boxes on the image. Argument: image: original image. boxes: ndarray, boxes of objects. classes: ndarray, classes of objects. scores: ndarray, scores of objects. all_classes: all classes name. ''' for box , score , cl in zip ( boxes , scores , classes ): x , y , w , h = box top = max ( 0 , np . floor ( x + 0.5 ) . astype ( int )) left = max ( 0 , np . floor ( y + 0.5 ) . astype ( int )) right = min ( image . shape [ 1 ], np . floor ( x + w + 0.5 ) . astype ( int )) bottom = min ( image . shape [ 0 ], np . floor ( y + h + 0.5 ) . astype ( int )) cv2 . rectangle ( image , ( top , left ), ( right , bottom ), ( 255 , 0 , 0 ), 2 ) cv2 . putText ( image , '{0} {1:.2f}' . format ( all_classes [ cl ], score ), ( top , left - 6 ), cv2 . FONT_HERSHEY_SIMPLEX , 0.6 , ( 0 , 0 , 255 ), 1 , cv2 . LINE_AA ) print ( 'class: {0}, score: {1:.2f}' . format ( all_classes [ cl ], score )) print ( 'box coordinate x,y,w,h: {0}' . format ( box ))","title":"Drawing on Images"},{"location":"basics/#wait-break","text":"This is not exactly pythonic, so it means it is not as easy to decipher. 0xFF is an 8 bit binary mask that forces the result from waitKey() to be an integer of maximum 255, which is what a character in the keyboard can go till. ord(char) returns the character in integers which will also be of maximum 255. Hence by comparing the integer to the ord(char) value, we can check for a key pressed event and break the loop. # stop when character \"q\" is pressed if cv2 . waitKey ( 0 ) & 0xFF == ord ( 'q' ): break # stop when \"ESC\" key is pressed if cv2 . waitKey ( 20 ) & 0xFF == 27 : break # Once script is done, its usually good practice to call this line # It closes all windows (just in case you have multiple windows called) cv2 . destroyAllWindows ()","title":"Wait &amp; Break"},{"location":"cnn/","text":"Convolutional Neural Networks","title":"CNN"},{"location":"cnn/#convolutional-neural-networks","text":"","title":"Convolutional Neural Networks"},{"location":"install/","text":"Installation Installations for computer vision can prove to tricky, from nvidia GPU to certain libraries compiled from other languages, it can turn out to extremely time-consuming. OpenCV Using pip install opencv-python installs the official version of cv2. However, there are some contribution libraries which are missing because of certain issues; to install a complete package, use pip install opencv-contrib-python instead.","title":"Installation"},{"location":"install/#installation","text":"Installations for computer vision can prove to tricky, from nvidia GPU to certain libraries compiled from other languages, it can turn out to extremely time-consuming.","title":"Installation"},{"location":"install/#opencv","text":"Using pip install opencv-python installs the official version of cv2. However, there are some contribution libraries which are missing because of certain issues; to install a complete package, use pip install opencv-contrib-python instead.","title":"OpenCV"},{"location":"transfer_learning/","text":"Transfer Learning For CNN, because of the huge research done, and the complexity in architecture, we can use existing ones, with pretrained weights. For transfer learning for image recognition, the defacto is imagenet, whereby we can specify it under the weights argument. EfficientNet EfficientNet is developed by Google in 2019. It is able to achieve a high accuracy with less parameters through a novel compound scaling method. import efficientnet.tfkeras as efn def model ( input_shape , classes ): ''' transfer learning from imagenet's weights, using Google's efficientnet7 architecture top layer (include_top) is removed as the number of classes is changed ''' base = efn . EfficientNetB7 ( input_shape = input_shape , weights = 'imagenet' , include_top = False ) model = Sequential () model . add ( base ) model . add ( GlobalAveragePooling2D ()) model . add ( Dense ( classes , activation = 'softmax' )) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return model # alternatively... def model ( input_shape , classes ): model = efn . EfficientNetB3 ( input_shape = input_shape , weights = 'imagenet' , include_top = False ) x = model . output x = Flatten ()( x ) x = Dropout ( 0.5 )( x ) output_layer = Dense ( classes , activation = 'softmax' )( x ) model = Model ( inputs = model . input , outputs = output_layer ) model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) return model YOLO YOLO stands for You Only Look Once.","title":"Transfer Learning"},{"location":"transfer_learning/#transfer-learning","text":"For CNN, because of the huge research done, and the complexity in architecture, we can use existing ones, with pretrained weights. For transfer learning for image recognition, the defacto is imagenet, whereby we can specify it under the weights argument.","title":"Transfer Learning"},{"location":"transfer_learning/#efficientnet","text":"EfficientNet is developed by Google in 2019. It is able to achieve a high accuracy with less parameters through a novel compound scaling method. import efficientnet.tfkeras as efn def model ( input_shape , classes ): ''' transfer learning from imagenet's weights, using Google's efficientnet7 architecture top layer (include_top) is removed as the number of classes is changed ''' base = efn . EfficientNetB7 ( input_shape = input_shape , weights = 'imagenet' , include_top = False ) model = Sequential () model . add ( base ) model . add ( GlobalAveragePooling2D ()) model . add ( Dense ( classes , activation = 'softmax' )) model . compile ( loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = [ 'accuracy' ]) return model # alternatively... def model ( input_shape , classes ): model = efn . EfficientNetB3 ( input_shape = input_shape , weights = 'imagenet' , include_top = False ) x = model . output x = Flatten ()( x ) x = Dropout ( 0.5 )( x ) output_layer = Dense ( classes , activation = 'softmax' )( x ) model = Model ( inputs = model . input , outputs = output_layer ) model . compile ( optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [ 'accuracy' ]) return model","title":"EfficientNet"},{"location":"transfer_learning/#yolo","text":"YOLO stands for You Only Look Once.","title":"YOLO"},{"location":"videos/","text":"Videos From Webcam import cv2 # Connects to your computer's default camera cap = cv2 . VideoCapture ( 0 ) while True : # Capture frame-by-frame ret , frame = cap . read () # Our operations on the frame come here gray = cv2 . cvtColor ( frame , cv2 . COLOR_BGR2GRAY ) cv2 . imshow ( 'frame' , gray ) # quit with \"q\" if cv2 . waitKey ( 1 ) & 0xFF == ord ( 'q' ): break # When everything done, release the capture and destroy the windows cap . release () cv2 . destroyAllWindows () From File import cv2 import time cap = cv2 . VideoCapture ( '../DATA/video_capture.mp4' ) # FRAMES PER SECOND fps = 25 # check for video file if cap . isOpened () == False : print ( \"Error opening the video file\" ) # While the video is opened while cap . isOpened (): # Read the video file ret , frame = cap . read () # If we got frames, show them. if ret == True : # Display the frame at same frame rate of recording time . sleep ( 1 / fps ) cv2 . imshow ( 'frame' , frame ) if cv2 . waitKey ( 25 ) & 0xFF == ord ( 'q' ): break # automatically break this whole loop if the video is over else : break cap . release () # Closes all the frames cv2 . destroyAllWindows ()","title":"Videos"},{"location":"videos/#videos","text":"","title":"Videos"},{"location":"videos/#from-webcam","text":"import cv2 # Connects to your computer's default camera cap = cv2 . VideoCapture ( 0 ) while True : # Capture frame-by-frame ret , frame = cap . read () # Our operations on the frame come here gray = cv2 . cvtColor ( frame , cv2 . COLOR_BGR2GRAY ) cv2 . imshow ( 'frame' , gray ) # quit with \"q\" if cv2 . waitKey ( 1 ) & 0xFF == ord ( 'q' ): break # When everything done, release the capture and destroy the windows cap . release () cv2 . destroyAllWindows ()","title":"From Webcam"},{"location":"videos/#from-file","text":"import cv2 import time cap = cv2 . VideoCapture ( '../DATA/video_capture.mp4' ) # FRAMES PER SECOND fps = 25 # check for video file if cap . isOpened () == False : print ( \"Error opening the video file\" ) # While the video is opened while cap . isOpened (): # Read the video file ret , frame = cap . read () # If we got frames, show them. if ret == True : # Display the frame at same frame rate of recording time . sleep ( 1 / fps ) cv2 . imshow ( 'frame' , frame ) if cv2 . waitKey ( 25 ) & 0xFF == ord ( 'q' ): break # automatically break this whole loop if the video is over else : break cap . release () # Closes all the frames cv2 . destroyAllWindows ()","title":"From File"}]}